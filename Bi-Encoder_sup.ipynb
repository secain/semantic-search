{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2d986c-b268-4726-92ed-1c2878e456c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer,  LoggingHandler, losses, models, util\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "\n",
    "#Check if dataset exsist. If not, download and extract  it\n",
    "data_path = './dureader/train/dual.train.tsv'\n",
    "\n",
    "\n",
    "#You can specify any huggingface/transformers pre-trained model here, for example, bert-base-uncased, roberta-base, xlm-roberta-base\n",
    "model_name = 'hfl/rbtl3'\n",
    "\n",
    "# Read the dataset\n",
    "train_batch_size = 64\n",
    "num_epochs = 6\n",
    "model_save_path = 'output/bi-encoder-sup_hlf-rbtl3'+'-'+datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dee210f1-ec7e-44cc-822a-db6e4e12deea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/rbtl3 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-09 00:15:39 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b7c2d5-85d4-44f3-9274-808ed97914d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-09 00:15:39 - Read train dataset\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Read train dataset\")\n",
    "train_samples = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        data =line.rstrip().split('\\t')\n",
    "        inp_example = InputExample(texts=[data[0][:256], data[2][:256], data[4][:256]])\n",
    "        train_samples.append(inp_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42969b7-c17e-46c2-99d4-d646825a9991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_train_samples = []\n",
    "# with open('./dureader/train/cross.train.demo.tsv', 'r', encoding='utf-8') as f:\n",
    "#     for line in f.readlines():\n",
    "#         data =line.rstrip().split('\\t')\n",
    "#         inp_example = InputExample(texts=[data[0][:256], data[2][:256]]), label=float(data[3])\n",
    "#         cross_train_samples.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41719255-5e07-4f5f-ac3d-e8bfd435459d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences: 871877\n",
      "Dev sentences: 8807\n",
      "Test sentences: 8896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_samples, test_samples = train_test_split(train_samples, test_size=0.01, random_state=13)\n",
    "train_samples, dev_samples = train_test_split(train_samples, test_size=0.01, random_state=13)\n",
    "\n",
    "print(\"Train sentences:\", len(train_samples))\n",
    "print(\"Dev sentences:\", len(dev_samples))\n",
    "print(\"Test sentences:\", len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c855d61-bee4-467e-b48b-b62c40ffa980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-09 00:15:51 - Warmup-steps: 8175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af4b5fd4d384c34bc72e17be1230043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be92b25250c4dcabf38dd8ef897bb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/13624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:706: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-09 01:23:52 - TripletEvaluator: Evaluating the model on dev dataset after epoch 0:\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "\n",
    "evaluator = TripletEvaluator.from_input_examples(dev_samples, name='dev')\n",
    "\n",
    "# Configure the training. We skip evaluation in this example\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs  * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], \n",
    "          optimizer_params={'lr': 5e-05}, \n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=15000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path, \n",
    "          use_amp=True)\n",
    "\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9daad-1bf3-4e99-80fe-12ea3a237666",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e08e63-6eef-4d05-8178-8dbd2a74eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator = TripletEvaluator.from_input_examples(test_samples, name='test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ae8f1-69b4-4a24-b1d0-6bc56f7798b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"shutdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f9acc-b8b3-4b33-b299-0460947db037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
