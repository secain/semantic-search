{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "470f4fdc-02f3-410f-aaae-e8d09b760640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models, util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import faiss\n",
    "\n",
    "model_name = './output/bi-encoder-sup_hlf-rbtl3-2022-06-09'\n",
    "dataset_path = './dureader-retrieval-test1/test1.json'\n",
    "\n",
    "embedding_cache_path = 'embeddings-hfl-rbtl3.pkl'\n",
    "embedding_size = 1024    #Size of embeddings\n",
    "top_k_hits = 50         #Output k hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4880ffc-46dd-406d-8736-de3f11b73b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b3c063-28fc-489f-9649-989a903bd109",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index('faiss.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02010ad6-0e42-412d-9ae7-3cf5f3c7fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embeddings = model.encode(list(questions.question.values))\n",
    "question_embeddings = question_embeddings / np.linalg.norm(question_embeddings, axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc18409e-dcec-418b-b64b-3e4072e8e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_json(dataset_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7bed54f-a6c8-4e1e-877d-12912779dceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce723b0f80641049aab709b5a298832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.set_num_threads(12)\n",
    "results_corpus_ids = []\n",
    "for question_index in trange(0, len(question_embeddings), 10): \n",
    "    question_embedding = question_embeddings[question_index:question_index+10]\n",
    "#     question_embedding = np.expand_dims(question_embedding, axis=0)\n",
    "    distances, corpus_ids = index.search(question_embedding, top_k_hits)\n",
    "    results_corpus_ids.extend(corpus_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "536aff76-2fb3-40d0-898a-24d66600fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_id = {}\n",
    "for _index, corpus_ids in zip(range(0, len(question_embeddings)), results_corpus_ids):\n",
    "    results_id[_index] = list(corpus_ids)\n",
    "    \n",
    "with open('results_id.pkl', \"wb\") as fOut:\n",
    "    pickle.dump(results_id, fOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04031467-e918-4ecc-9c08-723afa9b0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dureader/passage-collection/passage2id.map.json', 'r', encoding='utf-8') as f:\n",
    "    passage2id_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c3a1fb0-badb-48e9-b85b-390b077dad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for key, value in results_id.items():\n",
    "    question_id = questions.question_id[key]\n",
    "    corpus_ids = [passage2id_map[str(i)] for i in value]\n",
    "    results[question_id] = corpus_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6981fdfd-99ef-4948-95a1-2680baedb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recall_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61588020-04af-4ffe-ae3a-85e334b3d517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7472eb-d91f-4068-b404-ed2b3d922f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2e7381-d776-4a17-860a-1fd2aa5f1401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-computed embeddings from disc\n"
     ]
    }
   ],
   "source": [
    "print(\"Load pre-computed embeddings from disc\")\n",
    "with open(embedding_cache_path, \"rb\") as fIn:\n",
    "    cache_data = pickle.load(fIn)\n",
    "    corpus_sentences = cache_data['sentences']\n",
    "    corpus_embeddings = cache_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6810d89a-225e-40f4-b830-0cbf9fea6deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_id.pkl', \"rb\") as f:\n",
    "    results_id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae587761-6997-40b1-b612-9a256d320c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_cross = {}\n",
    "for key, value in results_id.items():\n",
    "    question = questions.question[key]\n",
    "    corpus = [corpus_sentences[i] for i in value]\n",
    "    results_for_cross[question] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b988cf-1c0f-469b-8796-70a938c8bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_for_cross.pkl', \"wb\") as fOut:\n",
    "    pickle.dump(results_for_cross, fOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff486c8e-b926-4fb6-b671-a1c422b771bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We extract corpus ids and scores for the first query\n",
    "# hits = [{'corpus_id': id, 'score': score} for id, score in zip(corpus_ids[0], distances[0])]\n",
    "# hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "# print(\"Input question:\", inp_question)\n",
    "# for hit in hits[0:top_k_hits]:\n",
    "#     print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))\n",
    "\n",
    "# # Approximate Nearest Neighbor (ANN) is not exact, it might miss entries with high cosine similarity\n",
    "# # Here, we compute the recall of ANN compared to the exact results\n",
    "# correct_hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k_hits)[0]\n",
    "# correct_hits_ids = set([hit['corpus_id'] for hit in correct_hits])\n",
    "\n",
    "# ann_corpus_ids = set([hit['corpus_id'] for hit in hits])\n",
    "# if len(ann_corpus_ids) != len(correct_hits_ids):\n",
    "#     print(\"Approximate Nearest Neighbor returned a different number of results than expected\")\n",
    "\n",
    "# recall = len(ann_corpus_ids.intersection(correct_hits_ids)) / len(correct_hits_ids)\n",
    "# print(\"\\nApproximate Nearest Neighbor Recall@{}: {:.2f}\".format(top_k_hits, recall * 100))\n",
    "\n",
    "# if recall < 1:\n",
    "#     print(\"Missing results:\")\n",
    "#     for hit in correct_hits[0:top_k_hits]:\n",
    "#         if hit['corpus_id'] not in ann_corpus_ids:\n",
    "#             print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))\n",
    "# print(\"\\n\\n========\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df8e1f-14e5-4139-8060-2b8df0583f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_samples = []\n",
    "# with open('./dureader/train/cross.train.tsv', 'r', encoding='utf-8') as f:\n",
    "#     for line in f.readlines():\n",
    "#         data = line.rstrip().split('\\t')\n",
    "#         inp_example = [data[0][:256], data[2][:256], float(data[3])]\n",
    "#         train_samples.append(inp_example)\n",
    "\n",
    "\n",
    "# emb1 = model.encode('测试')\n",
    "# emb2 = model.encode(inp_example[1])\n",
    "# print('label: ', inp_example[2])\n",
    "# print('sim: ', util.cos_sim(emb1, emb2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
